{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import numpy as np\
import pandas as pd\
from sklearn import cluster\
from scipy.cluster import hierarchy\
import numpy as np\
from matplotlib import cm\
from mpl_toolkits.mplot3d import Axes3D\
from matplotlib import pyplot as plt\
from scipy.cluster.hierarchy import dendrogram\
from skimage.measure import regionprops\
from scipy.ndimage.measurements import label\
from sklearn.metrics import calinski_harabaz_score as chindex\
from scipy.spatial.distance import euclidean, pdist, squareform\
\
\
\
def plot_dendrogram(model, **kwargs):\
\
    # Children of hierarchical clustering\
    children = model.children_\
\
    # Distances between each pair of children\
    # Since we don't have this information, we can use a uniform one for plotting\
    distance = np.arange(children.shape[0])\
\
    # The number of observations contained in each cluster level\
    no_of_observations = np.arange(2, children.shape[0]+2)\
\
    # Create linkage matrix and then plot the dendrogram\
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\
\
    # Plot the corresponding dendrogram\
    dendrogram(linkage_matrix, **kwargs)\
    \
dataLoc="/Users/paritoshgoel/Desktop/ruiz/project_4/absentieesim_workload.csv"\
data=pd.read_csv(dataLoc)\
print(data.columns)\
service_time = data.groupby(by = ['ID'])['Service_time'].mean()\
wl = data.groupby(by = ['ID'])['Work_load_Average/day_'].mean()\
hit_target = data.groupby(by = ['ID'])['Hit_target'].mean()\
d_f = data.groupby(by = ['ID'])['Disciplinary_failure'].mean()\
\
df_concat = pd.concat([service_time, wl, hit_target, d_f], axis=1)\
\
allEps=np.linspace(0.01,7.5)\
allMinSamps=np.arange(1,5)\
outliers=np.zeros((np.size(allEps),np.size(allMinSamps)))\
n_clusters=np.zeros((np.size(allEps),np.size(allMinSamps)))\
for k in range(np.size(allEps)):\
    eps=allEps[k]\
    for l in range(np.size(allMinSamps)):\
        min_samples=allMinSamps[l]\
        clustering=cluster.DBSCAN(eps=eps,min_samples=min_samples).fit(df_concat)\
        outliers[k,l]=np.sum(clustering.labels_==-1)\
        n_clusters[k,l]=np.amax(clustering.labels_)+1\
       \
\
clustering=cluster.DBSCAN(eps=500,min_samples=4).fit(df_concat)\
labels=clustering.labels_\
data2=df_concat.values[np.argsort(labels)]\
print(data2)\
plt.imshow(data2.dot(np.transpose(data2)))\
plt.show()\
\
dists = pdist(DF_var, similarity_func)\
plt.imshow(squareform(dists))\
\
fig = plt.figure()\
ax = fig.gca(projection='3d')\
X, Y = np.meshgrid(allEps, allMinSamps)\
surf=ax.plot_surface(X,Y,np.transpose(n_clusters),cmap=cm.coolwarm,\
                       linewidth=0, antialiased=False)\
fig.colorbar(surf, shrink=0.5, aspect=5)\
plt.gca().invert_xaxis()\
plt.gca().invert_yaxis()\
plt.show()\
\
fig = plt.figure()\
ax = fig.gca(projection='3d')\
X, Y = np.meshgrid(allEps, allMinSamps)\
surf=ax.plot_surface(X,Y,np.transpose(outliers),cmap=cm.coolwarm,\
                       linewidth=0, antialiased=False)\
fig.colorbar(surf, shrink=0.5, aspect=5)\
plt.show()}